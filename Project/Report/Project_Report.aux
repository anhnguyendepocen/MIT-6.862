\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{panch2018artificial}
\citation{Dua:2019}
\citation{street1993nuclear}
\citation{NickStreetWebsite}
\citation{scikit-learn,chen2016xgboost}
\newlabel{objective}{{1}{2}{}{equation.2.1}{}}
\newlabel{raw-image}{{1}{2}{Two raw fine-needle aspirate images of breast tissue from which features were computed. The left-hand example is benign and the right-hand example is malignant}{figure.1}{}}
\newlabel{bin-cross}{{2}{2}{}{equation.2.2}{}}
\citation{chollet2015keras}
\newlabel{ML-table}{{1}{3}{Cross-validated classification metrics for naive Bayes, logistic regression, and the XGBoost classifier. All values for metrics are scaled to be percentages. Any hyperparameters that were tuned during the validation process are included in the bottom row. For naive Bayes, this was \texttt {var\_smoothing}, the fraction of the largest variance of any feature that is added to the other variances for numerical stability. For XGBoost, $\lambda $ is \texttt {lambda\_reg}, the hyperparameter governing L2 regularization, and $n$ is \texttt {n\_estimators}, the number of trees to fit. For logistic regression, $C$ is equivalent to the inverse of $\lambda $ in L2 regularization. Only those hyperparameters mentioned were tuned, with any others left as their defaults}{table.1}{}}
\newlabel{small-cnn}{{2}{3}{Small CNN architecture trained from scratch. Each box corresponds to 32 5x5 filters being applied in a convolutional layer, using a ReLU activation and `same' padding, followed by a 2x2 max-pooling layer (not shown). The total number of trained parameters is 1,334,017. This output is flattened and then passed to a fully-connected layer with 128 units and a ReLU activation, to a single unit with sigmoid activation for class probability}{figure.2}{}}
\citation{raghu2019transfusion}
\bibdata{example_paper}
\bibcite{chen2016xgboost}{{1}{2016}{{Chen \& Guestrin}}{{Chen and Guestrin}}}
\bibcite{chollet2015keras}{{2}{2015}{{Chollet et~al.}}{{}}}
\bibcite{Dua:2019}{{3}{2017}{{Dua \& Graff}}{{Dua and Graff}}}
\bibcite{panch2018artificial}{{4}{2018}{{Panch et~al.}}{{Panch, Szolovits, and Atun}}}
\bibcite{scikit-learn}{{5}{2011}{{Pedregosa et~al.}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}}}
\newlabel{CNN-table}{{2}{4}{Cross-validated classification metrics for convolutional neural networks, both and using transfer learning. All values for metrics are scaled to be percentages. In each case, binary cross-entropy loss was used with stochastic gradient descent used for optimization with the default Keras hyperparameters}{table.2}{}}
\bibcite{raghu2019transfusion}{{6}{2019}{{Raghu et~al.}}{{Raghu, Zhang, Kleinberg, and Bengio}}}
\bibcite{NickStreetWebsite}{{7}{2019}{{Street}}{{}}}
\bibcite{street1993nuclear}{{8}{1993}{{Street et~al.}}{{Street, Wolberg, and Mangasarian}}}
\bibstyle{icml2019}
